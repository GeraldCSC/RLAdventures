{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bLay78IBDuG6",
    "outputId": "36b186b4-0c68-4385-ec5a-600092c508bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/gerald/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym) (1.3.2)\n",
      "Collecting cloudpickle~=1.2.0\n",
      "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym) (1.19.0)\n",
      "Requirement already satisfied: scipy in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: six in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: future in /home/gerald/anaconda3/lib/python3.7/site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.17.1)\n",
      "Installing collected packages: cloudpickle\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.5.0\n",
      "    Uninstalling cloudpickle-1.5.0:\n",
      "      Successfully uninstalled cloudpickle-1.5.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "spyder 3.3.6 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 3.3.6 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "spinup 0.2.0 requires cloudpickle==1.2.1, but you'll have cloudpickle 1.2.2 which is incompatible.\n",
      "spinup 0.2.0 requires matplotlib==3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\n",
      "spinup 0.2.0 requires torch==1.3.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
      "Successfully installed cloudpickle-1.2.2\n",
      "Requirement already satisfied: gym[atari] in /home/gerald/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.2.2)\n",
      "Requirement already satisfied: six in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.12.0)\n",
      "Requirement already satisfied: scipy in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (1.19.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /home/gerald/anaconda3/lib/python3.7/site-packages (from gym[atari]) (6.2.0)\n",
      "Collecting opencv-python; extra == \"atari\"\n",
      "  Downloading opencv_python-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (49.4 MB)\n",
      "\u001b[K     |███████████▌                    | 17.7 MB 129 kB/s eta 0:04:04^C\n",
      "\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrdict in /home/gerald/anaconda3/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied: six in /home/gerald/anaconda3/lib/python3.7/site-packages (from attrdict) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install gym[atari]\n",
    "!pip install attrdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEMr3-VED1Ct"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(reward_list):\n",
    "    \"\"\"\n",
    "        assuming that the rewards are put in chornological order\n",
    "    \"\"\"\n",
    "    rtg_list = []\n",
    "    tot_sum = 0\n",
    "    for item in reversed(reward_list):\n",
    "        tot_sum += item\n",
    "        rtg_list.append(tot_sum)\n",
    "    return list(reversed(rtg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QArrenRBD1yH"
   },
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "  def __init__(self, input_dim, action_space):\n",
    "    super(Policy_Network, self).__init__()\n",
    "    self.fc1 = nn.Sequential(nn.Linear(input_dim, 300), nn.ReLU())\n",
    "    self.fc2 = nn.Sequential(nn.Linear(300, 150), nn.ReLU())\n",
    "    self.fc3 = nn.Sequential(nn.Linear(150, action_space))\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJvOEx1GEpEP"
   },
   "outputs": [],
   "source": [
    "def get_action(logit):\n",
    "  m = Categorical(logits=logit)\n",
    "  return m.sample().item()\n",
    "\n",
    "def compute_loss(NN, obs, actions,rewards, batch_size):\n",
    "  logits = NN(obs)\n",
    "  logprob = Categorical(logits=logits).log_prob(actions)\n",
    "  return -(rewards * logprob).sum()/batch_size\n",
    "\n",
    "def train(env_name = \"Breakout-ram-v0\", batch_size = 50, num_epoch = 300):\n",
    "  env = gym.make(env_name)\n",
    "  dim_obs = env.observation_space.shape[0]\n",
    "  a_space = env.action_space.n\n",
    "  NN = Policy_Network(dim_obs, a_space).to(device)\n",
    "  optimizer = Adam(NN.parameters())\n",
    "  for i in range(num_epoch):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_rewards = []\n",
    "    ep_rewards = []\n",
    "    epoch_ret = []\n",
    "    while True:\n",
    "      batch_obs.append(obs.copy())\n",
    "      curr_action = get_action(NN(torch.as_tensor(obs, dtype=torch.float32, device = device)))\n",
    "      obs, rew, done, _ = env.step(curr_action)\n",
    "      batch_acts.append(curr_action)\n",
    "      ep_rewards.append(rew)\n",
    "      if done:\n",
    "        ep_ret , ep_len = sum(ep_rewards) , len(ep_rewards)\n",
    "        epoch_ret.append(ep_ret)\n",
    "        batch_rewards += reward_to_go(ep_rewards)\n",
    "        ep_rewards = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        if len(batch_obs) > batch_size:\n",
    "          break\n",
    "    batch_rewards = normalize([batch_rewards])\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(NN,torch.tensor(batch_obs, dtype=torch.float32, device = device),\n",
    "                                torch.as_tensor(batch_acts, dtype=torch.int32, device = device),\n",
    "                              torch.as_tensor(batch_rewards, dtype=torch.float32, device = device),\n",
    "                              batch_size)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    mean_ret = sum(epoch_ret) / len(epoch_ret)\n",
    "    print(\"Epoch: {} Epoch Avg Return: {}\".format(i, mean_ret))\n",
    "  return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZAwZBTnRUx99",
    "outputId": "353f4a69-b392-4d83-a064-17dfaba9ef1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Epoch Avg Return: 23.60141509433962\n",
      "Epoch: 1 Epoch Avg Return: 26.69333333333333\n",
      "Epoch: 2 Epoch Avg Return: 29.925373134328357\n",
      "Epoch: 3 Epoch Avg Return: 35.041811846689896\n",
      "Epoch: 4 Epoch Avg Return: 36.845588235294116\n",
      "Epoch: 5 Epoch Avg Return: 37.62406015037594\n",
      "Epoch: 6 Epoch Avg Return: 42.97424892703863\n",
      "Epoch: 7 Epoch Avg Return: 44.644444444444446\n",
      "Epoch: 8 Epoch Avg Return: 51.18877551020408\n",
      "Epoch: 9 Epoch Avg Return: 53.4468085106383\n",
      "Epoch: 10 Epoch Avg Return: 62.99375\n",
      "Epoch: 11 Epoch Avg Return: 66.24503311258277\n",
      "Epoch: 12 Epoch Avg Return: 78.578125\n",
      "Epoch: 13 Epoch Avg Return: 76.65648854961832\n",
      "Epoch: 14 Epoch Avg Return: 85.52991452991454\n",
      "Epoch: 15 Epoch Avg Return: 97.625\n",
      "Epoch: 16 Epoch Avg Return: 111.15555555555555\n",
      "Epoch: 17 Epoch Avg Return: 129.92207792207793\n",
      "Epoch: 18 Epoch Avg Return: 140.69444444444446\n",
      "Epoch: 19 Epoch Avg Return: 141.2112676056338\n",
      "Epoch: 20 Epoch Avg Return: 165.21311475409837\n",
      "Epoch: 21 Epoch Avg Return: 165.5737704918033\n",
      "Epoch: 22 Epoch Avg Return: 201.62\n",
      "Epoch: 23 Epoch Avg Return: 223.86666666666667\n",
      "Epoch: 24 Epoch Avg Return: 238.86046511627907\n",
      "Epoch: 25 Epoch Avg Return: 258.61538461538464\n",
      "Epoch: 26 Epoch Avg Return: 266.13157894736844\n",
      "Epoch: 27 Epoch Avg Return: 299.38235294117646\n",
      "Epoch: 28 Epoch Avg Return: 291.9428571428571\n",
      "Epoch: 29 Epoch Avg Return: 307.3030303030303\n",
      "Epoch: 30 Epoch Avg Return: 301.02941176470586\n",
      "Epoch: 31 Epoch Avg Return: 314.84375\n",
      "Epoch: 32 Epoch Avg Return: 320.5\n",
      "Epoch: 33 Epoch Avg Return: 304.2121212121212\n",
      "Epoch: 34 Epoch Avg Return: 394.15384615384613\n",
      "Epoch: 35 Epoch Avg Return: 329.93548387096774\n",
      "Epoch: 36 Epoch Avg Return: 347.7931034482759\n",
      "Epoch: 37 Epoch Avg Return: 347.3448275862069\n",
      "Epoch: 38 Epoch Avg Return: 386.81481481481484\n",
      "Epoch: 39 Epoch Avg Return: 397.11538461538464\n",
      "Epoch: 40 Epoch Avg Return: 394.5\n",
      "Epoch: 41 Epoch Avg Return: 388.22222222222223\n",
      "Epoch: 42 Epoch Avg Return: 352.3103448275862\n",
      "Epoch: 43 Epoch Avg Return: 343.4\n",
      "Epoch: 44 Epoch Avg Return: 415.56\n",
      "Epoch: 45 Epoch Avg Return: 403.2692307692308\n",
      "Epoch: 46 Epoch Avg Return: 379.55555555555554\n",
      "Epoch: 47 Epoch Avg Return: 371.8888888888889\n",
      "Epoch: 48 Epoch Avg Return: 457.3636363636364\n",
      "Epoch: 49 Epoch Avg Return: 433.5416666666667\n",
      "Epoch: 50 Epoch Avg Return: 467.6363636363636\n",
      "Epoch: 51 Epoch Avg Return: 443.5652173913044\n",
      "Epoch: 52 Epoch Avg Return: 476.6363636363636\n",
      "Epoch: 53 Epoch Avg Return: 489.9047619047619\n",
      "Epoch: 54 Epoch Avg Return: 471.95454545454544\n",
      "Epoch: 55 Epoch Avg Return: 476.1363636363636\n",
      "Epoch: 56 Epoch Avg Return: 447.5652173913044\n",
      "Epoch: 57 Epoch Avg Return: 447.39130434782606\n",
      "Epoch: 58 Epoch Avg Return: 469.59090909090907\n",
      "Epoch: 59 Epoch Avg Return: 451.6521739130435\n",
      "Epoch: 60 Epoch Avg Return: 486.0\n",
      "Epoch: 61 Epoch Avg Return: 462.1818181818182\n",
      "Epoch: 62 Epoch Avg Return: 483.85714285714283\n",
      "Epoch: 63 Epoch Avg Return: 499.0\n",
      "Epoch: 64 Epoch Avg Return: 484.04761904761904\n",
      "Epoch: 65 Epoch Avg Return: 487.85714285714283\n",
      "Epoch: 66 Epoch Avg Return: 483.1904761904762\n",
      "Epoch: 67 Epoch Avg Return: 490.3333333333333\n",
      "Epoch: 68 Epoch Avg Return: 485.42857142857144\n",
      "Epoch: 69 Epoch Avg Return: 495.8095238095238\n",
      "Epoch: 70 Epoch Avg Return: 497.8095238095238\n",
      "Epoch: 71 Epoch Avg Return: 485.2857142857143\n",
      "Epoch: 72 Epoch Avg Return: 473.22727272727275\n",
      "Epoch: 73 Epoch Avg Return: 484.76190476190476\n",
      "Epoch: 74 Epoch Avg Return: 457.77272727272725\n",
      "Epoch: 75 Epoch Avg Return: 480.85714285714283\n",
      "Epoch: 76 Epoch Avg Return: 444.17391304347825\n",
      "Epoch: 77 Epoch Avg Return: 438.17391304347825\n",
      "Epoch: 78 Epoch Avg Return: 446.4782608695652\n",
      "Epoch: 79 Epoch Avg Return: 450.1304347826087\n",
      "Epoch: 80 Epoch Avg Return: 453.7391304347826\n",
      "Epoch: 81 Epoch Avg Return: 468.77272727272725\n",
      "Epoch: 82 Epoch Avg Return: 437.04347826086956\n",
      "Epoch: 83 Epoch Avg Return: 454.04347826086956\n",
      "Epoch: 84 Epoch Avg Return: 442.04347826086956\n",
      "Epoch: 85 Epoch Avg Return: 436.625\n",
      "Epoch: 86 Epoch Avg Return: 479.76190476190476\n",
      "Epoch: 87 Epoch Avg Return: 448.0\n",
      "Epoch: 88 Epoch Avg Return: 400.5769230769231\n",
      "Epoch: 89 Epoch Avg Return: 439.1304347826087\n",
      "Epoch: 90 Epoch Avg Return: 473.8181818181818\n",
      "Epoch: 91 Epoch Avg Return: 435.6521739130435\n",
      "Epoch: 92 Epoch Avg Return: 462.90909090909093\n",
      "Epoch: 93 Epoch Avg Return: 468.27272727272725\n",
      "Epoch: 94 Epoch Avg Return: 464.6818181818182\n",
      "Epoch: 95 Epoch Avg Return: 461.90909090909093\n",
      "Epoch: 96 Epoch Avg Return: 485.57142857142856\n",
      "Epoch: 97 Epoch Avg Return: 487.3333333333333\n",
      "Epoch: 98 Epoch Avg Return: 499.8095238095238\n",
      "Epoch: 99 Epoch Avg Return: 500.0\n",
      "Epoch: 100 Epoch Avg Return: 499.0952380952381\n",
      "Epoch: 101 Epoch Avg Return: 498.6666666666667\n",
      "Epoch: 102 Epoch Avg Return: 488.5238095238095\n",
      "Epoch: 103 Epoch Avg Return: 447.1304347826087\n",
      "Epoch: 104 Epoch Avg Return: 428.75\n",
      "Epoch: 105 Epoch Avg Return: 475.5\n",
      "Epoch: 106 Epoch Avg Return: 476.5\n",
      "Epoch: 107 Epoch Avg Return: 476.0\n",
      "Epoch: 108 Epoch Avg Return: 489.76190476190476\n",
      "Epoch: 109 Epoch Avg Return: 476.22727272727275\n",
      "Epoch: 110 Epoch Avg Return: 496.04761904761904\n",
      "Epoch: 111 Epoch Avg Return: 442.17391304347825\n",
      "Epoch: 112 Epoch Avg Return: 478.23809523809524\n",
      "Epoch: 113 Epoch Avg Return: 485.6666666666667\n",
      "Epoch: 114 Epoch Avg Return: 496.23809523809524\n",
      "Epoch: 115 Epoch Avg Return: 488.6190476190476\n",
      "Epoch: 116 Epoch Avg Return: 492.6666666666667\n",
      "Epoch: 117 Epoch Avg Return: 490.0952380952381\n",
      "Epoch: 118 Epoch Avg Return: 500.0\n",
      "Epoch: 119 Epoch Avg Return: 500.0\n",
      "Epoch: 120 Epoch Avg Return: 485.0\n",
      "Epoch: 121 Epoch Avg Return: 500.0\n",
      "Epoch: 122 Epoch Avg Return: 497.14285714285717\n",
      "Epoch: 123 Epoch Avg Return: 489.3333333333333\n",
      "Epoch: 124 Epoch Avg Return: 470.0\n",
      "Epoch: 125 Epoch Avg Return: 468.45454545454544\n",
      "Epoch: 126 Epoch Avg Return: 485.5238095238095\n",
      "Epoch: 127 Epoch Avg Return: 477.57142857142856\n",
      "Epoch: 128 Epoch Avg Return: 496.6666666666667\n",
      "Epoch: 129 Epoch Avg Return: 500.0\n",
      "Epoch: 130 Epoch Avg Return: 500.0\n",
      "Epoch: 131 Epoch Avg Return: 500.0\n",
      "Epoch: 132 Epoch Avg Return: 496.0\n",
      "Epoch: 133 Epoch Avg Return: 490.14285714285717\n",
      "Epoch: 134 Epoch Avg Return: 488.6190476190476\n",
      "Epoch: 135 Epoch Avg Return: 500.0\n",
      "Epoch: 136 Epoch Avg Return: 489.2857142857143\n",
      "Epoch: 137 Epoch Avg Return: 490.0\n",
      "Epoch: 138 Epoch Avg Return: 458.3181818181818\n",
      "Epoch: 139 Epoch Avg Return: 476.42857142857144\n",
      "Epoch: 140 Epoch Avg Return: 476.3181818181818\n",
      "Epoch: 141 Epoch Avg Return: 477.1363636363636\n",
      "Epoch: 142 Epoch Avg Return: 486.3333333333333\n",
      "Epoch: 143 Epoch Avg Return: 466.0\n",
      "Epoch: 144 Epoch Avg Return: 480.6190476190476\n",
      "Epoch: 145 Epoch Avg Return: 479.4761904761905\n",
      "Epoch: 146 Epoch Avg Return: 465.40909090909093\n",
      "Epoch: 147 Epoch Avg Return: 438.5217391304348\n",
      "Epoch: 148 Epoch Avg Return: 462.09090909090907\n",
      "Epoch: 149 Epoch Avg Return: 476.3809523809524\n",
      "Epoch: 150 Epoch Avg Return: 495.6190476190476\n",
      "Epoch: 151 Epoch Avg Return: 466.5\n",
      "Epoch: 152 Epoch Avg Return: 466.09090909090907\n",
      "Epoch: 153 Epoch Avg Return: 490.3809523809524\n",
      "Epoch: 154 Epoch Avg Return: 455.30434782608694\n",
      "Epoch: 155 Epoch Avg Return: 487.0952380952381\n",
      "Epoch: 156 Epoch Avg Return: 491.2857142857143\n",
      "Epoch: 157 Epoch Avg Return: 479.2857142857143\n",
      "Epoch: 158 Epoch Avg Return: 489.14285714285717\n",
      "Epoch: 159 Epoch Avg Return: 488.14285714285717\n",
      "Epoch: 160 Epoch Avg Return: 484.3333333333333\n",
      "Epoch: 161 Epoch Avg Return: 472.09090909090907\n",
      "Epoch: 162 Epoch Avg Return: 438.0\n",
      "Epoch: 163 Epoch Avg Return: 439.04347826086956\n",
      "Epoch: 164 Epoch Avg Return: 483.5238095238095\n",
      "Epoch: 165 Epoch Avg Return: 482.0952380952381\n",
      "Epoch: 166 Epoch Avg Return: 476.3333333333333\n",
      "Epoch: 167 Epoch Avg Return: 465.22727272727275\n",
      "Epoch: 168 Epoch Avg Return: 420.6666666666667\n",
      "Epoch: 169 Epoch Avg Return: 404.72\n",
      "Epoch: 170 Epoch Avg Return: 400.04\n",
      "Epoch: 171 Epoch Avg Return: 419.75\n",
      "Epoch: 172 Epoch Avg Return: 436.8333333333333\n",
      "Epoch: 173 Epoch Avg Return: 463.90909090909093\n",
      "Epoch: 174 Epoch Avg Return: 492.0952380952381\n",
      "Epoch: 175 Epoch Avg Return: 499.5238095238095\n",
      "Epoch: 176 Epoch Avg Return: 499.9047619047619\n",
      "Epoch: 177 Epoch Avg Return: 499.57142857142856\n",
      "Epoch: 178 Epoch Avg Return: 480.6666666666667\n",
      "Epoch: 179 Epoch Avg Return: 499.0952380952381\n",
      "Epoch: 180 Epoch Avg Return: 487.4761904761905\n",
      "Epoch: 181 Epoch Avg Return: 495.42857142857144\n",
      "Epoch: 182 Epoch Avg Return: 488.85714285714283\n",
      "Epoch: 183 Epoch Avg Return: 464.3181818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 Epoch Avg Return: 436.5652173913044\n",
      "Epoch: 185 Epoch Avg Return: 441.82608695652175\n",
      "Epoch: 186 Epoch Avg Return: 461.40909090909093\n",
      "Epoch: 187 Epoch Avg Return: 487.7142857142857\n",
      "Epoch: 188 Epoch Avg Return: 497.04761904761904\n",
      "Epoch: 189 Epoch Avg Return: 496.42857142857144\n",
      "Epoch: 190 Epoch Avg Return: 495.2857142857143\n",
      "Epoch: 191 Epoch Avg Return: 480.95238095238096\n",
      "Epoch: 192 Epoch Avg Return: 497.5238095238095\n",
      "Epoch: 193 Epoch Avg Return: 489.9047619047619\n",
      "Epoch: 194 Epoch Avg Return: 487.04761904761904\n",
      "Epoch: 195 Epoch Avg Return: 500.0\n",
      "Epoch: 196 Epoch Avg Return: 500.0\n",
      "Epoch: 197 Epoch Avg Return: 488.6190476190476\n",
      "Epoch: 198 Epoch Avg Return: 467.59090909090907\n",
      "Epoch: 199 Epoch Avg Return: 487.14285714285717\n",
      "Epoch: 200 Epoch Avg Return: 466.54545454545456\n",
      "Epoch: 201 Epoch Avg Return: 405.32\n",
      "Epoch: 202 Epoch Avg Return: 475.22727272727275\n",
      "Epoch: 203 Epoch Avg Return: 475.77272727272725\n",
      "Epoch: 204 Epoch Avg Return: 461.59090909090907\n",
      "Epoch: 205 Epoch Avg Return: 471.59090909090907\n",
      "Epoch: 206 Epoch Avg Return: 445.39130434782606\n",
      "Epoch: 207 Epoch Avg Return: 487.76190476190476\n",
      "Epoch: 208 Epoch Avg Return: 475.1818181818182\n",
      "Epoch: 209 Epoch Avg Return: 480.3333333333333\n",
      "Epoch: 210 Epoch Avg Return: 485.8095238095238\n",
      "Epoch: 211 Epoch Avg Return: 496.76190476190476\n",
      "Epoch: 212 Epoch Avg Return: 480.7142857142857\n",
      "Epoch: 213 Epoch Avg Return: 482.0\n",
      "Epoch: 214 Epoch Avg Return: 469.40909090909093\n",
      "Epoch: 215 Epoch Avg Return: 486.85714285714283\n",
      "Epoch: 216 Epoch Avg Return: 498.42857142857144\n",
      "Epoch: 217 Epoch Avg Return: 484.3333333333333\n",
      "Epoch: 218 Epoch Avg Return: 492.7142857142857\n",
      "Epoch: 219 Epoch Avg Return: 493.5238095238095\n",
      "Epoch: 220 Epoch Avg Return: 488.5238095238095\n",
      "Epoch: 221 Epoch Avg Return: 480.95238095238096\n",
      "Epoch: 222 Epoch Avg Return: 493.7142857142857\n",
      "Epoch: 223 Epoch Avg Return: 334.96666666666664\n",
      "Epoch: 224 Epoch Avg Return: 255.55\n",
      "Epoch: 225 Epoch Avg Return: 209.5625\n",
      "Epoch: 226 Epoch Avg Return: 197.7843137254902\n",
      "Epoch: 227 Epoch Avg Return: 182.01785714285714\n",
      "Epoch: 228 Epoch Avg Return: 170.45762711864407\n",
      "Epoch: 229 Epoch Avg Return: 153.98461538461538\n",
      "Epoch: 230 Epoch Avg Return: 150.56716417910448\n",
      "Epoch: 231 Epoch Avg Return: 177.71929824561403\n",
      "Epoch: 232 Epoch Avg Return: 190.26415094339623\n",
      "Epoch: 233 Epoch Avg Return: 204.24489795918367\n",
      "Epoch: 234 Epoch Avg Return: 216.95744680851064\n",
      "Epoch: 235 Epoch Avg Return: 238.23809523809524\n",
      "Epoch: 236 Epoch Avg Return: 305.6666666666667\n",
      "Epoch: 237 Epoch Avg Return: 409.08\n",
      "Epoch: 238 Epoch Avg Return: 480.6190476190476\n",
      "Epoch: 239 Epoch Avg Return: 435.1304347826087\n",
      "Epoch: 240 Epoch Avg Return: 459.59090909090907\n",
      "Epoch: 241 Epoch Avg Return: 457.27272727272725\n",
      "Epoch: 242 Epoch Avg Return: 486.1904761904762\n",
      "Epoch: 243 Epoch Avg Return: 498.85714285714283\n",
      "Epoch: 244 Epoch Avg Return: 472.04545454545456\n",
      "Epoch: 245 Epoch Avg Return: 474.1818181818182\n",
      "Epoch: 246 Epoch Avg Return: 485.3809523809524\n",
      "Epoch: 247 Epoch Avg Return: 474.27272727272725\n",
      "Epoch: 248 Epoch Avg Return: 489.76190476190476\n",
      "Epoch: 249 Epoch Avg Return: 500.0\n"
     ]
    }
   ],
   "source": [
    "NN = train(env_name=\"CartPole-v1\",batch_size = 10000, num_epoch = 250)\n",
    "torch.save(NN.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vm33jTPsUzEj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
